import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from torch.optim import AdamW
from torch.optim.lr_scheduler import CosineAnnealingLR
from tqdm import tqdm
import os
import numpy as np
import math
import argparse

# å¼•å…¥é¡¹ç›®æ¨¡å—
from dataset import ManiaDataset
from model_v2 import ManiaDiffuserV2 

# ================= é…ç½®åŒºåŸŸ =================
DEVICE = "cuda:1" if torch.cuda.is_available() else "cpu"
BATCH_SIZE = 32
LR = 1e-4
EPOCHS = 100
TIMESTEPS = 1000 
DATA_DIR = "./processed_dataset" 
SAVE_DIR = "./checkpoints"

# é€šé“é…ç½® (83 = 80 Mel + 3 Onsets)
IN_CHANNELS = 4      
AUDIO_CHANNELS = 83  
BASE_DIM = 128       

# ===========================================

class DiffusionTrainer:
    def __init__(self, model, timesteps=1000):
        self.model = model
        self.timesteps = timesteps
        self.device = DEVICE
        self.beta = self.cosine_beta_schedule(timesteps).to(self.device)
        self.alpha = 1.0 - self.beta
        self.alpha_hat = torch.cumprod(self.alpha, dim=0)
        self.sqrt_alpha_hat = torch.sqrt(self.alpha_hat)
        self.sqrt_one_minus_alpha_hat = torch.sqrt(1. - self.alpha_hat)

    def cosine_beta_schedule(self, timesteps, s=0.008):
        steps = timesteps + 1
        x = torch.linspace(0, timesteps, steps)
        alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * math.pi * 0.5) ** 2
        alphas_cumprod = alphas_cumprod / alphas_cumprod[0]
        betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])
        return torch.clip(betas, 0.0001, 0.9999)

    def noise_images(self, x, t):
        sqrt_alpha_hat = self.sqrt_alpha_hat[t][:, None, None]
        sqrt_one_minus_alpha_hat = self.sqrt_one_minus_alpha_hat[t][:, None, None]
        epsilon = torch.randn_like(x)
        return sqrt_alpha_hat * x + sqrt_one_minus_alpha_hat * epsilon, epsilon

    def sample_timesteps(self, n):
        return torch.randint(low=1, high=self.timesteps, size=(n,), device=self.device)

    # def train_step(self, x_start, mel, sr):
    #     t = self.sample_timesteps(x_start.shape[0])
    #     x_noisy, noise = self.noise_images(x_start, t)
    #     predicted_noise = self.model(x_noisy, mel, t, sr)
    #     loss = F.mse_loss(predicted_noise, noise)
    #     return loss

    def train_step(self, x_start, mel, sr, prob_uncond=0.15): # 15% çš„æ¦‚çŽ‡ä¸¢å¼ƒ SR æ¡ä»¶
        t = self.sample_timesteps(x_start.shape[0])
        x_noisy, noise = self.noise_images(x_start, t)
        
        # å…‹éš†ä¸€ä¸‹ SRï¼Œé¿å…ä¿®æ”¹åŽŸå§‹æ•°æ®
        sr_input = sr.clone()
        
        # ç”ŸæˆæŽ©ç ï¼š15% çš„æ¦‚çŽ‡ä¸º True
        if prob_uncond > 0:
            mask = torch.rand(sr.shape[0], device=self.device) < prob_uncond
            # å°†è¢«é€‰ä¸­çš„ SR è®¾ä¸º -1.0 (ä»£è¡¨ Unconditional)
            # æ³¨æ„ï¼šLinearå±‚å¯ä»¥æŽ¥å—è´Ÿæ•°ï¼Œæ¨¡åž‹ä¼šè‡ªåŠ¨å­¦ä¹  -1 ä»£è¡¨â€œç©ºâ€
            sr_input[mask] = -1.0
        
        predicted_noise = self.model(x_noisy, mel, t, sr_input)
        loss = F.mse_loss(predicted_noise, noise)
        return loss

# === éªŒè¯ç›‘æŽ§å™¨ (ç®€åŒ–ç‰ˆ) ===
class ValidationMonitor:
    def __init__(self, diffusion_trainer, device):
        self.diffusion = diffusion_trainer
        self.device = device

    @torch.no_grad()
    def check(self, model, mel, sr, epoch):
        model.eval()
        # ç®€å•æ‰“å°æ—¥å¿—ï¼Œé¿å…åˆ·å±
        print(f"\n[Epoch {epoch}] Saving checkpoint and checking distribution...")
        model.train()

def main(args):
    if not os.path.exists(SAVE_DIR):
        os.makedirs(SAVE_DIR)
        
    print(f"Using device: {DEVICE}")
    
    # 1. æ•°æ®å‡†å¤‡
    dataset = ManiaDataset(DATA_DIR, sigma=1.5)
    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)
    
    # 2. æ¨¡åž‹åˆå§‹åŒ–
    model = ManiaDiffuserV2(
        in_channels=IN_CHANNELS,       
        audio_channels=AUDIO_CHANNELS, 
        base_dim=BASE_DIM,             
        dim_mults=(1, 2, 4, 8)         
    ).to(DEVICE)
    
    optimizer = AdamW(model.parameters(), lr=LR)
    scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS)
    
    diffusion = DiffusionTrainer(model, timesteps=TIMESTEPS)
    monitor = ValidationMonitor(diffusion, DEVICE)

    start_epoch = 0

    # ================= ç»­è®­é€»è¾‘ (Resume Logic) =================
    if args.resume:
        if os.path.isfile(args.resume):
            print(f"ðŸ”„ Loading checkpoint: {args.resume}")
            checkpoint = torch.load(args.resume, map_location=DEVICE)
            
            # åˆ¤æ–­æ˜¯æ—§ç‰ˆ(åªå­˜äº†æƒé‡)è¿˜æ˜¯æ–°ç‰ˆ(å­˜äº†å®Œæ•´çŠ¶æ€)
            if 'model_state_dict' in checkpoint:
                # å®Œæ•´çŠ¶æ€åŠ è½½
                model.load_state_dict(checkpoint['model_state_dict'])
                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
                scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
                start_epoch = checkpoint['epoch'] + 1
                print(f"âœ… Resumed from Epoch {start_epoch}")
            else:
                # æ—§ç‰ˆå…¼å®¹ï¼šåªåŠ è½½æƒé‡ï¼Œä¼˜åŒ–å™¨é‡ç½®
                model.load_state_dict(checkpoint)
                print(f"âš ï¸ Loaded weights only (Old Format). Restarting scheduler from Epoch 0.")
        else:
            print(f"âŒ Checkpoint not found: {args.resume}")
            return
    # ==========================================================

    # 3. è®­ç»ƒå¾ªçŽ¯
    for epoch in range(start_epoch, EPOCHS):
        model.train()
        pbar = tqdm(dataloader, desc=f"Epoch {epoch+1}/{EPOCHS}")
        avg_loss = 0
        
        for audio, chart, sr in pbar:
            audio = audio.to(DEVICE)
            chart = chart.to(DEVICE)
            sr = sr.to(DEVICE)
            
            optimizer.zero_grad()
            loss = diffusion.train_step(chart, audio, sr)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            
            avg_loss += loss.item()
            pbar.set_postfix(loss=f"{loss.item():.4f}", lr=f"{optimizer.param_groups[0]['lr']:.6f}")
        
        scheduler.step()
        
        # ä¿å­˜é€»è¾‘ï¼šä¿å­˜å®Œæ•´çŠ¶æ€å­—å…¸
        if (epoch + 1) % 5 == 0 or epoch == 0: # æ¯5è½®ä¿å­˜ä¸€æ¬¡
            save_path = os.path.join(SAVE_DIR, f"checkpoint_epoch_{epoch+1}.pt")
            
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'loss': avg_loss / len(dataloader)
            }, save_path)
            
            # å¦å¤–å­˜ä¸€ä¸ªåªå«æƒé‡çš„ best.pt æ–¹ä¾¿æŽ¨ç†è„šæœ¬ç›´æŽ¥è¯»å–
            torch.save(model.state_dict(), os.path.join(SAVE_DIR, "best.pt"))
            
            monitor.check(model, None, None, epoch+1)

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    # æ·»åŠ  --resume å‚æ•°ï¼Œå¦‚æžœä¸ä¼ åˆ™ä»Žå¤´å¼€å§‹
    parser.add_argument("--resume", type=str, default='checkpoints/checkpoint_epoch_25.pt', help="Path to checkpoint (.pt) to resume from")
    args = parser.parse_args()
    
    main(args)