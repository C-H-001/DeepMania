import torch
import torch.nn.functional as F
from torch.utils.data import DataLoader
from torch.optim import AdamW
from tqdm import tqdm
import os
import numpy as np

# å¼•å…¥ä¹‹å‰çš„ç±»
from dataset import ManiaDataset
from model import ManiaUNet

# ================= é…ç½®åŒºåŸŸ =================
DEVICE = "cuda:1" if torch.cuda.is_available() else "cpu"
BATCH_SIZE = 32 
LR = 1e-4
EPOCHS = 100
TIMESTEPS = 1200
DATA_DIR = "./processed_dataset" # ç¡®ä¿è¿™é‡ŒæŒ‡å‘ä½ æ–°çš„æ•°æ®é›†è·¯å¾„
SAVE_DIR = "./checkpoints"
# ===========================================

# === æ–°å¢ï¼šéªŒè¯ç›‘æ§å™¨ ===
class ValidationMonitor:
    def __init__(self, diffusion_trainer, device):
        self.diffusion = diffusion_trainer
        self.device = device

    @torch.no_grad()
    def check(self, model, mel, sr, epoch):
        """
        æ‰§è¡Œä¸€æ¬¡é‡‡æ ·å¹¶ç»Ÿè®¡åˆ†å¸ƒ
        mel: [1, 80, L]
        sr: [1, 1]
        """
        model.eval()
        print(f"\n[Epoch {epoch}] æ­£åœ¨ç”Ÿæˆé¢„è§ˆä»¥æ£€æŸ¥è½¨é“åˆ†å¸ƒ...")

        # 1. æ‰§è¡Œé‡‡æ · (ç®€åŒ–ç‰ˆï¼Œä¸ºäº†é€Ÿåº¦å¯ä»¥ä½¿ç”¨è¾ƒå°‘çš„æ­¥æ•°ï¼Œæˆ–è€…åšæŒ1000æ­¥ä»¥æ±‚å‡†ç¡®)
        # è¿™é‡Œç›´æ¥è°ƒç”¨ DiffusionTrainer é‡Œçš„é‡‡æ ·é€»è¾‘ï¼ˆæˆ‘ä»¬éœ€è¦å…ˆæŠŠå®ƒåŠ è¿›å»ï¼‰
        # å¦‚æœ Trainer é‡Œæ²¡æœ‰ sample æ–¹æ³•ï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œæ‰‹åŠ¨å†™ä¸€ä¸ªç®€åŒ–çš„
        
        b, c, l = 4, 4, mel.shape[2] # è¿™é‡Œçš„ b=4 æ˜¯æŒ‡æˆ‘ä»¬ç”Ÿæˆ 4 ä¸ªé€šé“
        img = torch.randn((1, 4, l), device=self.device)
        
        # ç®€åŒ–é‡‡æ ·è¿‡ç¨‹ï¼šä¸ºäº†ä¸æ‹–æ…¢è®­ç»ƒå¤ªå¤šï¼Œæˆ‘ä»¬å¯ä»¥åªç”¨ 100 æ­¥ (DDIM) æˆ–è€…å®Œæ•´è·‘å®Œ
        # ä¸ºäº†å‡†ç¡®çœ‹åˆ°æ˜¯å¦ collapseï¼Œå»ºè®®å®Œæ•´è·‘å®Œï¼Œæˆ–è€…è‡³å°‘è·‘ 200 æ­¥
        # è¿™é‡Œå¤ç”¨æ ‡å‡†çš„ DDPM é‡‡æ ·
        steps = self.diffusion.timesteps
        for i in reversed(range(0, steps)):
            t = torch.full((1,), i, device=self.device, dtype=torch.long)
            predicted_noise = model(img, mel, t, sr)
            
            alpha = self.diffusion.alpha[t][:, None, None]
            alpha_hat = self.diffusion.alpha_hat[t][:, None, None]
            beta = self.diffusion.beta[t][:, None, None]
            
            if i > 0:
                noise = torch.randn_like(img)
            else:
                noise = torch.zeros_like(img)
                
            img = (1 / torch.sqrt(alpha)) * (img - ((1 - alpha) / (torch.sqrt(1 - alpha_hat))) * predicted_noise) + torch.sqrt(beta) * noise

        # 2. ç»Ÿè®¡åˆ†å¸ƒ
        # å½’ä¸€åŒ–å› [0, 1]
        img = (img.clamp(-1, 1) + 1) / 2
        grid = img[0].cpu().numpy() # [4, Length]
        
        # è®¾å®šé˜ˆå€¼åˆ¤æ–­ Note
        threshold = 0.55
        notes = (grid > threshold).astype(int)
        
        # æŒ‰è½¨é“æ±‚å’Œ
        counts = np.sum(notes, axis=1) # [Count_Col1, Count_Col2, ...]
        total_notes = np.sum(counts) + 1e-8 # é˜²æ­¢é™¤ä»¥0
        
        # 3. æ‰“å°ç»“æœ
        print("-" * 40)
        print(f"ğŸ“Š åˆ†å¸ƒç»Ÿè®¡ (Target SR: {sr.item():.1f})")
        print(f"   Total Notes: {int(total_notes)}")
        
        max_notes = np.max(counts)
        for k in range(4):
            count = counts[k]
            ratio = count / total_notes * 100
            # ç®€å•çš„ ASCII æ¡å½¢å›¾
            bar_len = int((count / max_notes) * 20) if max_notes > 0 else 0
            bar = "â–ˆ" * bar_len
            print(f"   Track {k+1}: {count:5d} ({ratio:5.1f}%) | {bar}")
            
        # æ£€æŸ¥å¹³è¡¡æ€§ (æ ‡å‡†å·®)
        std_dev = np.std(counts / total_notes)
        if std_dev < 0.05:
            print("âœ… è½¨é“åˆ†å¸ƒå‡è¡¡ (Good Balance)")
        elif std_dev > 0.15:
            print("âš ï¸ è½¨é“åˆ†å¸ƒæåº¦ä¸å‡ (Possible Mode Collapse)")
            
        print("-" * 40)
        
        model.train() # åˆ‡å›è®­ç»ƒæ¨¡å¼

# === æ‰©æ•£æ¨¡å‹è¾…åŠ©ç±» (è¡¥å…… sample éœ€è¦çš„å‚æ•°) ===
class DiffusionTrainer:
    def __init__(self, model, timesteps=1000):
        self.model = model
        self.timesteps = timesteps
        self.beta = torch.linspace(1e-4, 0.02, timesteps).to(DEVICE)
        self.alpha = 1.0 - self.beta
        self.alpha_hat = torch.cumprod(self.alpha, dim=0)
        
        # é¢„è®¡ç®—ä¸€äº›å‚æ•°ç”¨äº q_sample
        self.sqrt_alpha_hat = torch.sqrt(self.alpha_hat)
        self.sqrt_one_minus_alpha_hat = torch.sqrt(1. - self.alpha_hat)

    def noise_images(self, x, t):
        """ åŠ å™ªè¿‡ç¨‹ q(x_t | x_0) """
        # x å¿…é¡»æ˜¯ [-1, 1] èŒƒå›´
        sqrt_alpha_hat = self.sqrt_alpha_hat[t][:, None, None]
        sqrt_one_minus_alpha_hat = self.sqrt_one_minus_alpha_hat[t][:, None, None]
        epsilon = torch.randn_like(x)
        return sqrt_alpha_hat * x + sqrt_one_minus_alpha_hat * epsilon, epsilon

    # åœ¨ DiffusionTrainer.train_step ä¸­

    # ä¿®æ”¹ DiffusionTrainer çš„ train_step

    def train_step(self, x_start, mel, sr):
        t = self.sample_timesteps(x_start.shape[0])
        x_noisy, noise = self.noise_images(x_start, t)
        predicted_noise = self.model(x_noisy, mel, t, sr)
        
        # --- å›å½’æœ€åŸå§‹çš„ MSE Loss ---
        # æ—¢ç„¶æˆ‘ä»¬å·²ç»ç”¨äº†é«˜æ–¯çƒ­åŠ›å›¾ï¼Œtarget æœ¬èº«å°±æ˜¯å¹³æ»‘çš„
        # ä¸éœ€è¦é¢å¤–çš„ weight æ¥å¼ºåˆ¶å®ƒå­¦ä¹  Note
        loss = F.mse_loss(predicted_noise, noise)
        
        return loss

    def sample_timesteps(self, n):
        return torch.randint(low=1, high=self.timesteps, size=(n,), device=DEVICE)
# === ä¸»è®­ç»ƒå¾ªç¯ ===
def main():
    if not os.path.exists(SAVE_DIR):
        os.makedirs(SAVE_DIR)
        
    print(f"Using device: {DEVICE}")
    
    # 1. æ•°æ®å‡†å¤‡ (å¼€å¯ Augment!)
    dataset = ManiaDataset(DATA_DIR, sigma=2.0)
    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True)
    
    # 2. æ¨¡å‹åˆå§‹åŒ–
    model = ManiaUNet(in_channels=4, audio_channels=81, base_dim=64).to(DEVICE)
    optimizer = AdamW(model.parameters(), lr=LR)
    diffusion = DiffusionTrainer(model, timesteps=TIMESTEPS)
    monitor = ValidationMonitor(diffusion, DEVICE)
    
    # === æŠ½å–ä¸€ä¸ªå›ºå®šçš„éªŒè¯æ ·æœ¬ ===
    # æˆ‘ä»¬ä»æ•°æ®é›†é‡Œå–ç¬¬ä¸€ä¸ªæ ·æœ¬ï¼Œä¸€ç›´ç”¨å®ƒæ¥è§‚å¯Ÿæ¨¡å‹å˜åŒ–
    sample_data = dataset[0] 
    # å¢åŠ  Batch ç»´åº¦ [1, ...]
    fixed_mel = sample_data[0].unsqueeze(0).to(DEVICE) 
    fixed_sr = sample_data[2].unsqueeze(0).to(DEVICE)
    print(f"Validation Sample Loaded. Target SR: {fixed_sr.item()}")

    # 3. å¼€å§‹è®­ç»ƒ
    for epoch in range(EPOCHS):
        model.train()
        pbar = tqdm(dataloader, desc=f"Epoch {epoch+1}/{EPOCHS}")
        avg_loss = 0
        
        for mel, chart, sr in pbar:
            mel = mel.to(DEVICE)
            chart = chart.to(DEVICE)
            sr = sr.to(DEVICE)
            
            optimizer.zero_grad()
            loss = diffusion.train_step(chart, mel, sr)
            loss.backward()
            optimizer.step()
            
            avg_loss += loss.item()
            pbar.set_postfix(loss=loss.item())
            
        print(f"Epoch {epoch+1} Average Loss: {avg_loss / len(dataloader):.6f}")
        
        # === æ¯ä¸ª Epoch ç»“æŸæ—¶è¿›è¡Œç›‘æ§ ===
        monitor.check(model, fixed_mel, fixed_sr, epoch+1)
        
        # ä¿å­˜æ¨¡å‹
        if (epoch + 1) % 5 == 0:
            torch.save(model.state_dict(), os.path.join(SAVE_DIR, f"model_epoch_{epoch+1}.pt"))

if __name__ == "__main__":
    main()