import argparse
import torch
import numpy as np
import librosa
import os
import math
import scipy.signal
from tqdm import tqdm
from model import ManiaUNet
import sr_calculator
# ================= é…ç½® =================
CONFIG = {
    'SR': 22050,
    'HOP_LENGTH': 512,
    'N_FFT': 2048,
    'N_MELS': 80,
    'KEYS': 4,
    'DEVICE': "cuda" if torch.cuda.is_available() else "cpu",
    'COLUMN_X': [64, 192, 320, 448] 
}

# ã€å…³é”®é…ç½®ã€‘å¿…é¡»ä¸ dataset.py ä¸­çš„ SHIFT_FRAMES ä¿æŒä¸€è‡´
# 3å¸§ * 23.2ms â‰ˆ 70ms çš„ç‰©ç†å»¶è¿Ÿè¡¥å¿
TRAIN_SHIFT_FRAMES = 2.8 

# ================= æ ¸å¿ƒç±»ä¸å‡½æ•° =================

class DiffusionSampler:
    def __init__(self, model, checkpoint_path, timesteps=1000):
        self.model = model
        self.timesteps = timesteps
        self.device = CONFIG['DEVICE']
        
        print(f"Loading model from {checkpoint_path}...")
        state_dict = torch.load(checkpoint_path, map_location=self.device)
        self.model.load_state_dict(state_dict)
        self.model.eval()
        
        self.beta = torch.linspace(1e-4, 0.02, timesteps).to(self.device)
        self.alpha = 1.0 - self.beta
        self.alpha_hat = torch.cumprod(self.alpha, dim=0)
        # ä¿®æ­£ alpha_hat_prev çš„è®¡ç®—ï¼Œé˜²æ­¢ç»´åº¦é”™è¯¯
        self.alpha_hat_prev = torch.cat([torch.tensor([1.0]).to(self.device), self.alpha_hat[:-1]])

    @torch.no_grad()
    def sample(self, audio_input, target_sr):
        b, c, l = audio_input.shape
        img = torch.randn((b, 4, l), device=self.device)
        sr_tensor = torch.tensor([target_sr], dtype=torch.float32).to(self.device)
        
        for i in tqdm(reversed(range(0, self.timesteps)), desc="Sampling", total=self.timesteps):
            t = torch.full((b,), i, device=self.device, dtype=torch.long)
            predicted_noise = self.model(img, audio_input, t, sr_tensor)
            
            alpha = self.alpha[t][:, None, None]
            alpha_hat = self.alpha_hat[t][:, None, None]
            beta = self.beta[t][:, None, None]
            
            if i > 0:
                noise = torch.randn_like(img)
            else:
                noise = torch.zeros_like(img)
            
            # æ ‡å‡† DDPM é‡‡æ ·å…¬å¼
            img = (1 / torch.sqrt(alpha)) * (img - ((1 - alpha) / (torch.sqrt(1 - alpha_hat))) * predicted_noise) + torch.sqrt(beta) * noise
            
        img = (img.clamp(-1, 1) + 1) / 2
        return img.cpu().numpy()

def parse_manual_timing(timing_str):
    """è§£æ timing å­—ç¬¦ä¸²ï¼Œå¤„ç†æ½œåœ¨çš„å¼•å·æˆ–ç©ºæ ¼é—®é¢˜"""
    try:
        clean_str = timing_str.replace('"', '').replace("'", "").strip()
        parts = clean_str.split(',')
        offset = float(parts[0])
        beat_len = float(parts[1])
        bpm = 60000.0 / beat_len
        print(f"Manual Timing Parsed: Offset={offset}ms, BPM={bpm:.2f}")
        return offset, beat_len
    except Exception as e:
        print(f"Error parsing timing string: {timing_str}")
        print("Format should be: 'Offset,BeatLength,...' (e.g. '1020,500,4...')")
        raise e

def prepare_audio_with_onset(audio_path):
    y, sr = librosa.load(audio_path, sr=CONFIG['SR'])
    
    # 1. Mel Spectrogram
    melspec = librosa.feature.melspectrogram(
        y=y, sr=sr, n_fft=CONFIG['N_FFT'], 
        hop_length=CONFIG['HOP_LENGTH'], n_mels=CONFIG['N_MELS']
    )
    log_mel = librosa.power_to_db(melspec, ref=np.max)
    
    # 2. Onset Strength
    onset_env = librosa.onset.onset_strength(S=log_mel, sr=sr)
    
    # Onset Normalization [-1, 1]
    if onset_env.max() > 0:
        onset_env = onset_env / onset_env.max() * 2.0 - 1.0
    else:
        onset_env = np.zeros_like(onset_env) - 1.0
    
    # Mel Normalization [-1, 1]
    norm_mel = log_mel / 40.0 + 1.0
    
    # 3. Concat & Pad
    mel_tensor = torch.tensor(norm_mel, dtype=torch.float32).unsqueeze(0)
    onset_tensor = torch.tensor(onset_env, dtype=torch.float32).unsqueeze(0).unsqueeze(0)
    
    combined_audio = torch.cat([mel_tensor, onset_tensor], dim=1).to(CONFIG['DEVICE'])
    length = combined_audio.shape[2]
    pad_len = math.ceil(length / 32) * 32 - length
    if pad_len > 0:
        combined_audio = torch.nn.functional.pad(combined_audio, (0, pad_len))
        
    return combined_audio

# ================= èŠ‚å¥é‡åŒ–é€»è¾‘ (æ ¸å¿ƒä¿®æ”¹) =================

def get_best_snap(time_ms, offset, beat_len, divisors):
    # è®¡ç®—ç›¸å¯¹äºæœ€è¿‘çº¢çº¿çš„åç§»
    rel_time = time_ms - offset
    
    # å¦‚æœåœ¨çº¢çº¿ä¹‹å‰ï¼Œä¸åšé‡åŒ–ç›´æ¥è¿”å› (æˆ–é‡åŒ–åˆ°è´Ÿæ•°æ‹)
    # è¿™é‡Œç®€å•å¤„ç†ï¼šå…è®¸è´Ÿæ•°ï¼Œä¿æŒæ•°å­¦è¿ç»­æ€§
    beat_pos = rel_time / beat_len
    
    best_time = time_ms
    min_err = float('inf')
    
    for div in divisors:
        snapped_pos = np.round(beat_pos * div) / div
        candidate_time = offset + snapped_pos * beat_len
        err = abs(candidate_time - time_ms)
        
        # æƒ©ç½šé¡¹ï¼šä¼˜å…ˆå¸é™„åˆ°ç®€å•çš„èŠ‚æ‹ (åˆ†æ¯è¶Šå°æƒ©ç½šè¶Šå°)
        # div=1 -> penalty=0.05
        # div=4 -> penalty=0.2
        # div=16 -> penalty=0.8
        weighted_err = err + (div * 0.05) 
        
        if weighted_err < min_err:
            min_err = weighted_err
            best_time = candidate_time
            
    return best_time, min_err

# ================= å‡çº§ç‰ˆï¼šå°èŠ‚é”å®šé‡åŒ– =================

def get_group_error(times, offset, beat_len, divisors):
    """è®¡ç®—ä¸€ç»„éŸ³ç¬¦åœ¨ç‰¹å®š rhythm set ä¸‹çš„æ€»æ‹Ÿåˆè¯¯å·®"""
    total_error = 0
    for t in times:
        # è®¡ç®—ç›¸å¯¹ä½ç½®
        rel = t - offset
        beat_pos = rel / beat_len
        
        min_dist = float('inf')
        for div in divisors:
            # æ‰¾æœ€è¿‘çš„ç½‘æ ¼
            snapped_pos = round(beat_pos * div) / div
            candidate_time = offset + snapped_pos * beat_len
            dist = abs(candidate_time - t)
            
            # è·ç¦»æƒ©ç½š + å¤æ‚åº¦æƒ©ç½š (divè¶Šå°è¶Šå¥½)
            # æˆ‘ä»¬å¸Œæœ›å°½é‡å¸é™„åˆ° 1/1, 1/2, 1/4
            score = dist + (div * 0.1) 
            if score < min_dist:
                min_dist = score
        total_error += min_dist
    return total_error

def snap_time_to_divisors(t, offset, beat_len, divisors):
    """æ‰§è¡Œå…·ä½“çš„å¸é™„æ“ä½œ"""
    rel = t - offset
    beat_pos = rel / beat_len
    
    best_time = t
    min_dist = float('inf')
    
    for div in divisors:
        snapped_pos = round(beat_pos * div) / div
        candidate_time = offset + snapped_pos * beat_len
        
        # è¿™é‡Œåªçœ‹çº¯ç‰©ç†è·ç¦»ï¼Œå› ä¸ºä¸Šé¢å·²ç»å†³å®šäº†ç”¨å“ªå¥—divisors
        dist = abs(candidate_time - t)
        if dist < min_dist:
            min_dist = dist
            best_time = candidate_time
            
    return best_time

def quantize_measure_wise(raw_notes, beat_len, offset, target_sr):
    """
    ä»¥å°èŠ‚(4æ‹)ä¸ºå•ä½è¿›è¡Œä¸€è‡´æ€§é”å®šã€‚
    è§£å†³ 1/3 å’Œ 1/4 æ··ç”¨çš„é—®é¢˜ã€‚
    """
    # --- 1. å®šä¹‰èŠ‚å¥é›†åˆ ---
    # åŸºç¡€ Straight
    divs_straight = [1, 2, 4]
    # åŸºç¡€ Swing
    divs_swing = [1, 2, 3] # 1/2 æ˜¯å…±ç”¨çš„ï¼Œä½†è¿™ä¸å½±å“åŒºåˆ†
    
    # é«˜éš¾åº¦ä¸‹çš„æ‰©å±•
    if target_sr >= 5.5:
        divs_straight += [8]    # Stream
        divs_swing += [6]       # Fast Swing
        
    if target_sr >= 6:
        divs_straight += [16]
        divs_swing += [12]

    print(f"Quantizer: Straight={divs_straight}, Swing={divs_swing}")

    # --- 2. æŒ‰å°èŠ‚åˆ†ç»„ (å‡è®¾ 4/4 æ‹) ---
    measure_len = beat_len * 4
    measure_groups = {} # key: measure_index, value: list of raw_times
    note_map = []       # store (time, col)
    
    for t, k in raw_notes:
        rel = t - offset
        # å‘ä¸‹å–æ•´åˆ°ç¬¬å‡ ä¸ªå°èŠ‚
        m_idx = int(rel / measure_len)
        if m_idx not in measure_groups: measure_groups[m_idx] = []
        measure_groups[m_idx].append(t)
        note_map.append({'time': t, 'col': k})

    final_snapped_map = {}

    # --- 3. å°èŠ‚çº§ç«äº‰ (Winner Takes All) ---
    # è¿™é‡Œæ˜¯æ ¸å¿ƒï¼šSwing Bias
    # åªæœ‰å½“ SwingError < StraightError * 0.65 æ—¶ï¼Œæ‰åˆ‡ Swing
    # æ„å‘³ç€ Swing å¿…é¡»æ¯” Straight å‡†å¾ˆå¤šæ‰è¡Œ
    SWING_THRESHOLD_RATIO = 0.65
    
    for m_idx, times in measure_groups.items():
        if not times: continue
        
        # æ’é™¤æ‰é‚£äº›æ˜æ˜¾æ˜¯ 1/1 æˆ– 1/2 çš„éŸ³ç¬¦ï¼ˆå®ƒä»¬å¯¹åˆ†è¾¨èŠ‚å¥æ²¡å¸®åŠ©ï¼Œæ˜¯å¹²æ‰°é¡¹ï¼‰
        # æˆ‘ä»¬åªå…³æ³¨é‚£äº›è½åœ¨ beat ä¸­é—´çš„éŸ³ç¬¦
        complex_notes = []
        for t in times:
            rel = (t - offset) / beat_len
            # å¦‚æœç¦»æ•´æ•°æ‹å¾ˆè¿‘ (<0.1æ‹)ï¼Œè¯´æ˜æ˜¯æ­£æ‹ï¼Œä¸å‚ä¸æŠ•ç¥¨
            if abs(rel - round(rel)) > 0.1:
                complex_notes.append(t)
        
        # å¦‚æœä¸€ä¸ªå°èŠ‚é‡Œå…¨æ˜¯æ­£æ‹(1/1)ï¼Œé‚£é»˜è®¤ä¸º Straight
        if not complex_notes:
            active_divs = divs_straight
        else:
            # è®¡ç®—è¿™ä¸€ç»„å¤æ‚éŸ³ç¬¦çš„è¯¯å·®
            err_str = get_group_error(complex_notes, offset, beat_len, divs_straight)
            err_swg = get_group_error(complex_notes, offset, beat_len, divs_swing)
            
            # åˆ¤å®š
            if err_swg < err_str * SWING_THRESHOLD_RATIO:
                active_divs = divs_swing
                # debug_info = "SWING (Locked)"
            else:
                active_divs = divs_straight
                # debug_info = "STRAIGHT"
            
            # print(f"Measure {m_idx}: StrErr={err_str:.1f}, SwgErr={err_swg:.1f} -> {debug_info}")

        # --- 4. æ‰§è¡Œå¸é™„ ---
        # å¯¹è¯¥å°èŠ‚å†…çš„æ‰€æœ‰éŸ³ç¬¦ï¼Œå¼ºåˆ¶ä½¿ç”¨é€‰å®šçš„ active_divs
        for t in times:
            snapped = snap_time_to_divisors(t, offset, beat_len, active_divs)
            final_snapped_map[t] = snapped

    # --- 5. ç»„è£…è¾“å‡º ---
    processed_objects = []
    seen = set()
    
    for item in note_map:
        raw_t = item['time']
        k = item['col']
        snapped_t = final_snapped_map.get(raw_t, int(raw_t))
        
        # å†æ¬¡å–æ•´ä¿è¯æ•´æ•°
        snapped_t = int(round(snapped_t))
        if snapped_t < 0: continue
        
        x = CONFIG['COLUMN_X'][k]
        key = (snapped_t, x)
        
        if key not in seen:
            seen.add(key)
            line = f"{x},192,{snapped_t},1,0,0:0:0:0:"
            processed_objects.append((snapped_t, line))
            
    processed_objects.sort(key=lambda x: x[0])
    return [x[1] for x in processed_objects]
def grid_to_hitobjects(grid, beat_len, offset, target_sr, threshold=0.5):
    raw_notes = [] 
    frame_ms = CONFIG['HOP_LENGTH'] / CONFIG['SR'] * 1000
    
    # ã€å…³é”®ä¿®æ­£ã€‘è®¡ç®—éœ€è¦æ‰£é™¤çš„ç‰©ç†æ—¶é—´åç§»é‡
    # ä¿®æ­£ Librosa Padding å¯¼è‡´çš„æ•´ä½“å‘å³åç§»
    offset_correction = TRAIN_SHIFT_FRAMES * frame_ms
    
    for k in range(4):
        signal = grid[k]
        # height: å³°å€¼é«˜åº¦
        # distance: æœ€å°é—´éš”å¸§æ•° (2å¸§çº¦ä¸º46msï¼Œé˜²æ­¢é‡å )
        peaks, _ = scipy.signal.find_peaks(signal, height=threshold, distance=2)
        
        for p_frame in peaks:
            # åŸå§‹æ—¶é—´ = å¸§ç´¢å¼•æ—¶é—´ - è®­ç»ƒæ—¶çš„äººä¸ºåç§»
            raw_time = p_frame * frame_ms - offset_correction
            raw_notes.append((raw_time, k))
            
    if not raw_notes: return []
    
    # è¿›å…¥èŠ‚å¥é‡åŒ–æµç¨‹
    return quantize_measure_wise(raw_notes, beat_len, offset, target_sr)

def write_osu_file(output_path, audio_filename, hit_objects, sr_val, timing_str):
    # æ¸…ç† timing string çš„æ ¼å¼
    timing_str = timing_str.replace('"', '').strip()
    
    content = f"""osu file format v14

[General]
AudioFilename: {audio_filename}
AudioLeadIn: 0
PreviewTime: -1
Countdown: 0
SampleSet: Soft
StackLeniency: 0.7
Mode: 3
LetterboxInBreaks: 0
WidescreenStoryboard: 0

[Metadata]
Title: AI Generated
TitleUnicode: AI Generated
Artist: DeepMania
ArtistUnicode: DeepMania
Creator: AI
Version: {sr_val} Stars
Source:
Tags:
BeatmapID: 0
BeatmapSetID: 0

[Difficulty]
HPDrainRate: 8
CircleSize: 4
OverallDifficulty: 8
ApproachRate: 5
SliderMultiplier: 1.4
SliderTickRate: 1

[Events]
//Background and Video events
//Break Periods
//Storyboard Layer 0 (Background)
//Storyboard Layer 1 (Fail)
//Storyboard Layer 2 (Pass)
//Storyboard Layer 3 (Foreground)
//Storyboard Sound Samples

[TimingPoints]
{timing_str}

[HitObjects]
""" 
    with open(output_path, "w", encoding="utf-8") as f:
        f.write(content)
        for line in hit_objects:
            f.write(line + "\n")
            
    print(f"Saved beatmap to {output_path}")

# ================= ä¸»å…¥å£ =================

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="DeepMania: AI Osu!Mania 4K Generator")
    parser.add_argument("--audio", type=str, required=True, help="Path to input MP3 file")
    parser.add_argument("--model", type=str, required=True, help="Path to model checkpoint (.pt)")
    parser.add_argument("--timing", type=str, required=True, help="Timing point string (e.g. '100,333.33,4...')")
    parser.add_argument("--sr", type=float, default=3.5, help="Target Star Rating (default: 3.5)")
    parser.add_argument("--out", type=str, default="output.osu", help="Output .osu file path")
    parser.add_argument("--threshold", type=float, default=0.55, help="Note detection threshold (0.0-1.0)")
    
    args = parser.parse_args()

    # 1. è§£æ Timing
    offset, beat_len = parse_manual_timing(args.timing)
    
    # 2. åˆå§‹åŒ–æ¨¡å‹ (ç¡®ä¿æ˜¯ 81 é€šé“)
    model = ManiaUNet(in_channels=4, audio_channels=81, base_dim=64).to(CONFIG['DEVICE'])
    sampler = DiffusionSampler(model, args.model, timesteps=1000)
    
    # 3. å‡†å¤‡éŸ³é¢‘
    print(f"Processing {args.audio}...")
    audio_tensor = prepare_audio_with_onset(args.audio)
    
    # 4. ç”Ÿæˆ Grid
    print(f"Generating for SR={args.sr}...")
    generated_grid = sampler.sample(audio_tensor, args.sr)[0]
    
    # 5. åå¤„ç† (å« Shift Correction å’Œ Rhythm Competition)
    print("Applying Rhythm Quantization...")
    hit_objects = grid_to_hitobjects(generated_grid, beat_len, offset, args.sr, threshold=args.threshold)
    
    # 6. å†™å…¥æ–‡ä»¶ (å…ˆä¿å­˜ä¸€ä¸ªä¸´æ—¶æ–‡ä»¶å)
    audio_filename = os.path.basename(args.audio)
    # ä½¿ç”¨ args.out ä½œä¸ºåˆå§‹è·¯å¾„
    temp_output_path = args.out 
    write_osu_file(temp_output_path, audio_filename, hit_objects, args.sr, args.timing)
    
    # ã€æ–°å¢ã€‘è®¡ç®—å®é™… SR å¹¶é‡å‘½å
    # ==========================================
    print("-" * 30)
    print("Calculating Actual Star Rating...")
    

    # è°ƒç”¨ sr_calculator
    # æ³¨æ„ï¼šæ ¹æ®ä¹‹å‰çš„ä¿®æ”¹ï¼Œä½ çš„ calculate å‡½æ•°åº”è¯¥è¿”å› (sr, df_corners)
    # æˆ‘ä»¬åªéœ€è¦ç¬¬ä¸€ä¸ªè¿”å›å€¼
    real_sr, _ = sr_calculator.calculate(temp_output_path, mod="")
    
    print(f"ğŸ¯ Target SR: {args.sr}")
    print(f"ğŸ“Š Actual SR: {real_sr:.2f}")

    # å¯é€‰ï¼šæ ¹æ®å®é™… SR é‡å‘½åæ–‡ä»¶
    # ä¾‹å¦‚: output.osu -> output_3.52.osu
    base, ext = os.path.splitext(temp_output_path)
    # ä¸ºäº†é˜²æ­¢æ–‡ä»¶åé‡å¤ï¼Œä¿ç•™ä¸€éƒ¨åˆ†åŸå
    new_filename = f"{base}_Real{real_sr:.2f}{ext}"
    
    # é‡å‘½åæ–‡ä»¶
    if os.path.exists(new_filename):
        os.remove(new_filename) # è¦†ç›–æ—§çš„
    os.rename(temp_output_path, new_filename)
    print(f"âœ… Renamed file to: {new_filename}")
    